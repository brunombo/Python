{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMCKUnQgq5oD3rCdcOV+4wa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brunombo/Python/blob/master/long_TTS_xtts_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "s8NfbT3sw2-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_to_speech_to_synthetise= \"Ce document pr√©sente les Manifold-Constrained Hyper-Connections (mHC), une architecture novatrice con√ßue par DeepSeek-AI pour stabiliser l'entra√Ænement des grands mod√®les de langage. Bien que les Hyper-Connections (HC) classiques am√©liorent les performances en √©largissant le flux r√©siduel, leur nature non contrainte provoque souvent une instabilit√© num√©rique et des probl√®mes de divergence du signal. Pour rem√©dier √† cela, les auteurs utilisent l'algorithme de Sinkhorn-Knopp afin de projeter les connexions sur une vari√©t√© de matrices doublement stochastiques, pr√©servant ainsi la propri√©t√© de mappage d'identit√©. Cette approche garantit une propagation saine du signal tout en optimisant l'efficacit√© mat√©rielle gr√¢ce √† la fusion de noyaux et √† des strat√©gies de m√©morisation s√©lective. Les r√©sultats exp√©rimentaux d√©montrent que mHC surpasse les m√©thodes existantes en termes de scalabilit√© et de capacit√©s de raisonnement sur divers tests de r√©f√©rence. En int√©grant ces contraintes g√©om√©triques rigoureuses, le cadre mHC offre une solution robuste pour l'√©volution des architectures neuronales √† grande √©chelle.\"\n",
        "\n",
        "voice_gender = 'female_fr'\n",
        "# ['female_fr', 'male_fr']"
      ],
      "metadata": {
        "id": "XYDOUW523oJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIKtDA5hweJP"
      },
      "outputs": [],
      "source": [
        "# Installation des d√©pendances\n",
        "!pip install -q scipy noisereduce\n",
        "!pip install -q numpy==2.0.2\n",
        "\n",
        "# Installation du fork maintenu (supporte Python 3.12+)\n",
        "!pip install -q coqui-tts\n",
        "!pip install -q torchcodec"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torchcodec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXsHJUGXH7_4",
        "outputId": "da0c3b45-bc9a-490a-8597-bbcf7e2eaf25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchcodec in /usr/local/lib/python3.12/dist-packages (0.9.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "TTS XTTS v2 - Version Long Audio (> 1 heure)\n",
        "=============================================\n",
        "\n",
        "Module de synth√®se vocale haute qualit√© utilisant Coqui XTTS v2.\n",
        "Optimis√© pour la g√©n√©ration d'audio longs avec:\n",
        "- enable_text_splitting=True pour d√©coupage automatique\n",
        "- Chunking intelligent par paragraphes pour textes tr√®s longs\n",
        "- Concat√©nation audio avec crossfade\n",
        "- Barre de progression et estimation temps restant\n",
        "- Gestion m√©moire optimis√©e\n",
        "- Correction du bug d'argument 'language' sur l'API synthesizer\n",
        "\n",
        "Auteur: Bruno\n",
        "Date: Janvier 2025\n",
        "Correction: Gemini\n",
        "\"\"\"\n",
        "\n",
        "# ==============================================================================\n",
        "# IMPORTS\n",
        "# ==============================================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import re\n",
        "import gc\n",
        "import wave\n",
        "import time\n",
        "import hashlib\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from typing import Optional, Union, List, Callable\n",
        "from dataclasses import dataclass\n",
        "from enum import Enum\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# ==============================================================================\n",
        "# INSTALLATION (Colab)\n",
        "# ==============================================================================\n",
        "\n",
        "def install_dependencies():\n",
        "    \"\"\"Installe les d√©pendances si n√©cessaire (Colab).\"\"\"\n",
        "    import subprocess\n",
        "    import sys\n",
        "\n",
        "    packages = [\n",
        "        (\"scipy\", \"scipy\"),\n",
        "        (\"noisereduce\", \"noisereduce\"),\n",
        "        (\"TTS\", \"coqui-tts\"),\n",
        "    ]\n",
        "\n",
        "    for module, package in packages:\n",
        "        try:\n",
        "            __import__(module)\n",
        "        except ImportError:\n",
        "            print(f\"üì¶ Installation de {package}...\")\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
        "\n",
        "    # numpy compatible\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numpy==2.0.2\"])\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "# ==============================================================================\n",
        "# CONFIGURATION\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class TTSConfig:\n",
        "    \"\"\"Configuration globale du module TTS.\"\"\"\n",
        "    MODEL_NAME: str = \"tts_models/multilingual/multi-dataset/xtts_v2\"\n",
        "    SAMPLE_RATE: int = 24000\n",
        "    DEFAULT_LANGUAGE: str = \"fr\"\n",
        "    GDRIVE_FOLDER: str = \"/content/drive/MyDrive/TTS_Output\"\n",
        "\n",
        "    # Configuration pour audio longs\n",
        "    MAX_CHARS_PER_CHUNK: int = 500  # Caract√®res max par chunk pour textes tr√®s longs\n",
        "    CROSSFADE_DURATION: float = 0.05  # Dur√©e du crossfade en secondes\n",
        "    ENABLE_TEXT_SPLITTING: bool = True  # Activer le split natif XTTS\n",
        "\n",
        "    PRESET_VOICES: dict = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.PRESET_VOICES = {\n",
        "            \"female_fr\": \"https://huggingface.co/spaces/coqui/xtts/resolve/main/examples/female.wav\",\n",
        "            \"male_fr\": \"https://huggingface.co/spaces/coqui/xtts/resolve/main/examples/male.wav\",\n",
        "        }\n",
        "\n",
        "Config = TTSConfig()\n",
        "\n",
        "# ==============================================================================\n",
        "# DEVICE MANAGEMENT\n",
        "# ==============================================================================\n",
        "\n",
        "_device = None\n",
        "_device_name = \"cpu\"\n",
        "\n",
        "def detect_device():\n",
        "    \"\"\"D√©tecte le meilleur device disponible.\"\"\"\n",
        "    global _device, _device_name\n",
        "    import torch\n",
        "\n",
        "    # Essayer TPU\n",
        "    try:\n",
        "        import torch_xla.core.xla_model as xm\n",
        "        _device = xm.xla_device()\n",
        "        _device_name = \"tpu\"\n",
        "        print(f\"‚öôÔ∏è Device: TPU\")\n",
        "        return\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Essayer CUDA\n",
        "    if torch.cuda.is_available():\n",
        "        _device = torch.device(\"cuda\")\n",
        "        _device_name = f\"cuda ({torch.cuda.get_device_name(0)})\"\n",
        "        print(f\"‚öôÔ∏è Device: {_device_name}\")\n",
        "        return\n",
        "\n",
        "    # Fallback CPU\n",
        "    _device = torch.device(\"cpu\")\n",
        "    _device_name = \"cpu\"\n",
        "    print(f\"‚öôÔ∏è Device: CPU\")\n",
        "\n",
        "# ==============================================================================\n",
        "# TEXT SPLITTING UTILITIES\n",
        "# ==============================================================================\n",
        "\n",
        "class TextSplitter:\n",
        "    \"\"\"\n",
        "    Utilitaire pour d√©couper intelligemment les textes longs.\n",
        "    Pr√©serve la coh√©rence des phrases et paragraphes.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def estimate_audio_duration(text: str, chars_per_second: float = 15.0) -> float:\n",
        "        \"\"\"\n",
        "        Estime la dur√©e audio pour un texte donn√©.\n",
        "        \"\"\"\n",
        "        return len(text) / chars_per_second\n",
        "\n",
        "    @staticmethod\n",
        "    def split_into_sentences(text: str) -> List[str]:\n",
        "        \"\"\"D√©coupe le texte en phrases.\"\"\"\n",
        "        # Pattern pour fin de phrase\n",
        "        pattern = r'(?<=[.!?])\\s+'\n",
        "        sentences = re.split(pattern, text)\n",
        "        return [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "    @staticmethod\n",
        "    def split_into_paragraphs(text: str) -> List[str]:\n",
        "        \"\"\"D√©coupe le texte en paragraphes.\"\"\"\n",
        "        paragraphs = re.split(r'\\n\\s*\\n', text)\n",
        "        return [p.strip() for p in paragraphs if p.strip()]\n",
        "\n",
        "    @classmethod\n",
        "    def split_for_long_audio(\n",
        "        cls,\n",
        "        text: str,\n",
        "        max_chars: int = 500,\n",
        "        preserve_sentences: bool = True\n",
        "    ) -> List[str]:\n",
        "        \"\"\"\n",
        "        D√©coupe un texte long en chunks optimaux pour la synth√®se.\n",
        "        \"\"\"\n",
        "        # Si texte court, retourner tel quel\n",
        "        if len(text) <= max_chars:\n",
        "            return [text]\n",
        "\n",
        "        chunks = []\n",
        "\n",
        "        if preserve_sentences:\n",
        "            sentences = cls.split_into_sentences(text)\n",
        "            current_chunk = \"\"\n",
        "\n",
        "            for sentence in sentences:\n",
        "                # Si la phrase seule d√©passe max_chars, la d√©couper\n",
        "                if len(sentence) > max_chars:\n",
        "                    if current_chunk:\n",
        "                        chunks.append(current_chunk.strip())\n",
        "                        current_chunk = \"\"\n",
        "                    # D√©couper la phrase longue par mots\n",
        "                    words = sentence.split()\n",
        "                    sub_chunk = \"\"\n",
        "                    for word in words:\n",
        "                        if len(sub_chunk) + len(word) + 1 <= max_chars:\n",
        "                            sub_chunk += \" \" + word if sub_chunk else word\n",
        "                        else:\n",
        "                            if sub_chunk:\n",
        "                                chunks.append(sub_chunk.strip())\n",
        "                            sub_chunk = word\n",
        "                    if sub_chunk:\n",
        "                        current_chunk = sub_chunk\n",
        "                elif len(current_chunk) + len(sentence) + 1 <= max_chars:\n",
        "                    current_chunk += \" \" + sentence if current_chunk else sentence\n",
        "                else:\n",
        "                    if current_chunk:\n",
        "                        chunks.append(current_chunk.strip())\n",
        "                    current_chunk = sentence\n",
        "\n",
        "            if current_chunk:\n",
        "                chunks.append(current_chunk.strip())\n",
        "        else:\n",
        "            # D√©coupage simple par caract√®res\n",
        "            for i in range(0, len(text), max_chars):\n",
        "                chunks.append(text[i:i + max_chars])\n",
        "\n",
        "        return chunks\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# AUDIO PROCESSING\n",
        "# ==============================================================================\n",
        "\n",
        "class AudioProcessor:\n",
        "    \"\"\"Processeur audio pour post-traitement et concat√©nation.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize(audio: np.ndarray, target_db: float = -3.0) -> np.ndarray:\n",
        "        \"\"\"Normalise l'audio au niveau cible.\"\"\"\n",
        "        if audio.dtype == np.int16:\n",
        "            audio = audio.astype(np.float32) / 32768.0\n",
        "\n",
        "        peak = np.max(np.abs(audio))\n",
        "        if peak > 0:\n",
        "            target_linear = 10 ** (target_db / 20)\n",
        "            audio = audio * (target_linear / peak)\n",
        "\n",
        "        return np.clip(audio, -1.0, 1.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def crossfade(\n",
        "        audio1: np.ndarray,\n",
        "        audio2: np.ndarray,\n",
        "        sample_rate: int,\n",
        "        duration: float = 0.05\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Concat√®ne deux segments audio avec crossfade.\n",
        "        \"\"\"\n",
        "        # Convertir en float si n√©cessaire\n",
        "        if audio1.dtype == np.int16:\n",
        "            audio1 = audio1.astype(np.float32) / 32768.0\n",
        "        if audio2.dtype == np.int16:\n",
        "            audio2 = audio2.astype(np.float32) / 32768.0\n",
        "\n",
        "        fade_samples = int(sample_rate * duration)\n",
        "\n",
        "        # Si audio trop court pour crossfade, concat√©ner simplement\n",
        "        if len(audio1) < fade_samples or len(audio2) < fade_samples:\n",
        "            return np.concatenate([audio1, audio2])\n",
        "\n",
        "        # Cr√©er les courbes de fade\n",
        "        fade_out = np.linspace(1.0, 0.0, fade_samples)\n",
        "        fade_in = np.linspace(0.0, 1.0, fade_samples)\n",
        "\n",
        "        # Appliquer le crossfade\n",
        "        audio1_end = audio1[-fade_samples:] * fade_out\n",
        "        audio2_start = audio2[:fade_samples] * fade_in\n",
        "\n",
        "        # Assembler\n",
        "        result = np.concatenate([\n",
        "            audio1[:-fade_samples],\n",
        "            audio1_end + audio2_start,\n",
        "            audio2[fade_samples:]\n",
        "        ])\n",
        "\n",
        "        return result\n",
        "\n",
        "    @classmethod\n",
        "    def concatenate_chunks(\n",
        "        cls,\n",
        "        audio_chunks: List[np.ndarray],\n",
        "        sample_rate: int,\n",
        "        crossfade_duration: float = 0.05\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Concat√®ne plusieurs chunks audio avec crossfade.\n",
        "        \"\"\"\n",
        "        if not audio_chunks:\n",
        "            return np.array([], dtype=np.float32)\n",
        "\n",
        "        if len(audio_chunks) == 1:\n",
        "            audio = audio_chunks[0]\n",
        "            if audio.dtype == np.int16:\n",
        "                audio = audio.astype(np.float32) / 32768.0\n",
        "            return audio\n",
        "\n",
        "        result = audio_chunks[0]\n",
        "        if result.dtype == np.int16:\n",
        "            result = result.astype(np.float32) / 32768.0\n",
        "\n",
        "        for chunk in audio_chunks[1:]:\n",
        "            result = cls.crossfade(result, chunk, sample_rate, crossfade_duration)\n",
        "\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def enhance(\n",
        "        audio: np.ndarray,\n",
        "        sample_rate: int,\n",
        "        normalize: bool = True,\n",
        "        warmth: bool = True\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"Am√©liore la qualit√© audio.\"\"\"\n",
        "        if audio.dtype == np.int16:\n",
        "            audio = audio.astype(np.float32) / 32768.0\n",
        "\n",
        "        if warmth:\n",
        "            try:\n",
        "                from scipy import signal\n",
        "                nyquist = sample_rate / 2\n",
        "                cutoff = min(300, nyquist * 0.9) / nyquist\n",
        "                b, a = signal.butter(2, cutoff, btype='low')\n",
        "                bass = signal.filtfilt(b, a, audio)\n",
        "                audio = audio + 0.15 * bass\n",
        "            except ImportError:\n",
        "                pass\n",
        "\n",
        "        if normalize:\n",
        "            peak = np.max(np.abs(audio))\n",
        "            if peak > 0:\n",
        "                target = 10 ** (-3.0 / 20)\n",
        "                audio = audio * (target / peak)\n",
        "\n",
        "        audio = np.clip(audio, -1.0, 1.0)\n",
        "        return audio\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# PROGRESS TRACKER\n",
        "# ==============================================================================\n",
        "\n",
        "class ProgressTracker:\n",
        "    \"\"\"Suivi de progression avec estimation du temps restant.\"\"\"\n",
        "\n",
        "    def __init__(self, total: int, description: str = \"\"):\n",
        "        self.total = total\n",
        "        self.current = 0\n",
        "        self.description = description\n",
        "        self.start_time = time.time()\n",
        "        self.chunk_times = []\n",
        "\n",
        "    def update(self, chunk_duration: float = None):\n",
        "        \"\"\"Met √† jour la progression.\"\"\"\n",
        "        self.current += 1\n",
        "        if chunk_duration:\n",
        "            self.chunk_times.append(chunk_duration)\n",
        "        self._display()\n",
        "\n",
        "    def _display(self):\n",
        "        \"\"\"Affiche la barre de progression.\"\"\"\n",
        "        elapsed = time.time() - self.start_time\n",
        "        percent = (self.current / self.total) * 100\n",
        "\n",
        "        # Estimation temps restant\n",
        "        if self.chunk_times:\n",
        "            avg_time = np.mean(self.chunk_times)\n",
        "            remaining = avg_time * (self.total - self.current)\n",
        "            eta_str = self._format_time(remaining)\n",
        "        else:\n",
        "            eta_str = \"...\"\n",
        "\n",
        "        # Barre de progression\n",
        "        bar_length = 30\n",
        "        filled = int(bar_length * self.current / self.total)\n",
        "        bar = \"‚ñà\" * filled + \"‚ñë\" * (bar_length - filled)\n",
        "\n",
        "        elapsed_str = self._format_time(elapsed)\n",
        "\n",
        "        print(f\"\\r{self.description} [{bar}] {self.current}/{self.total} \"\n",
        "              f\"({percent:.1f}%) | Temps: {elapsed_str} | ETA: {eta_str}\", end=\"\")\n",
        "\n",
        "        if self.current >= self.total:\n",
        "            print()  # Nouvelle ligne √† la fin\n",
        "\n",
        "    @staticmethod\n",
        "    def _format_time(seconds: float) -> str:\n",
        "        \"\"\"Formate un temps en secondes en HH:MM:SS.\"\"\"\n",
        "        hours = int(seconds // 3600)\n",
        "        minutes = int((seconds % 3600) // 60)\n",
        "        secs = int(seconds % 60)\n",
        "\n",
        "        if hours > 0:\n",
        "            return f\"{hours:02d}:{minutes:02d}:{secs:02d}\"\n",
        "        return f\"{minutes:02d}:{secs:02d}\"\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# TTS ENGINE\n",
        "# ==============================================================================\n",
        "\n",
        "_tts_model = None\n",
        "_voices_cache = {}\n",
        "os.environ[\"COQUI_TOS_AGREED\"] = \"1\"\n",
        "\n",
        "def get_model():\n",
        "    \"\"\"Charge le mod√®le XTTS v2 avec cache.\"\"\"\n",
        "    global _tts_model\n",
        "\n",
        "    if _tts_model is None:\n",
        "        print(\"üîÑ Chargement du mod√®le XTTS v2...\")\n",
        "        from TTS.api import TTS\n",
        "\n",
        "        _tts_model = TTS(Config.MODEL_NAME)\n",
        "\n",
        "        if _device is not None and _device_name.startswith(\"cuda\"):\n",
        "            _tts_model = _tts_model.to(_device)\n",
        "\n",
        "        print(\"‚úì Mod√®le charg√©\")\n",
        "\n",
        "    return _tts_model\n",
        "\n",
        "\n",
        "def get_voice_path(voice: str) -> str:\n",
        "    \"\"\"Obtient le chemin vers un fichier de voix.\"\"\"\n",
        "    global _voices_cache\n",
        "    import urllib.request\n",
        "\n",
        "    if voice in _voices_cache:\n",
        "        return _voices_cache[voice]\n",
        "\n",
        "    if os.path.isfile(voice):\n",
        "        _voices_cache[voice] = voice\n",
        "        return voice\n",
        "\n",
        "    if voice in Config.PRESET_VOICES:\n",
        "        url = Config.PRESET_VOICES[voice]\n",
        "        path = f\"/tmp/{voice}.wav\"\n",
        "\n",
        "        if not os.path.exists(path):\n",
        "            print(f\"üì• T√©l√©chargement de la voix '{voice}'...\")\n",
        "            urllib.request.urlretrieve(url, path)\n",
        "\n",
        "        _voices_cache[voice] = path\n",
        "        return path\n",
        "\n",
        "    raise FileNotFoundError(f\"Voix '{voice}' non trouv√©e\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# MAIN SYNTHESIS FUNCTIONS\n",
        "# ==============================================================================\n",
        "\n",
        "def synthesize_chunk(\n",
        "    text: str,\n",
        "    voice_path: str,\n",
        "    language: str = \"fr\",\n",
        "    enable_text_splitting: bool = True\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Synth√©tise un chunk de texte en audio via l'inf√©rence directe (Low-Level).\n",
        "    Bypass total du SpeakerManager pour √©viter le bug FileNotFoundError .pth\n",
        "    \"\"\"\n",
        "    model_wrapper = get_model()\n",
        "\n",
        "    # 1. Acc√®s \"chirurgical\" au mod√®le interne XTTS\n",
        "    # C'est lui qui fait le travail, sans la couche de gestion de fichiers bugg√©e\n",
        "    if hasattr(model_wrapper, 'synthesizer'):\n",
        "        xtts_model = model_wrapper.synthesizer.tts_model\n",
        "    else:\n",
        "        # Cas rare ou structure diff√©rente, on tente l'acc√®s direct\n",
        "        xtts_model = model_wrapper.tts_model\n",
        "\n",
        "    # 2. Calcul manuel des latents (Empreinte vocale)\n",
        "    # On transforme le fichier WAV en vecteurs math√©matiques\n",
        "    try:\n",
        "        gpt_cond_latent, speaker_embedding = xtts_model.get_conditioning_latents(\n",
        "            audio_path=[voice_path],\n",
        "            gpt_cond_len=30,\n",
        "            max_ref_length=60\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Erreur calcul latents: {e}\")\n",
        "        raise e\n",
        "\n",
        "    # 3. Inf√©rence directe\n",
        "    # On appelle la fonction de g√©n√©ration pure, sans passer par tts()\n",
        "    try:\n",
        "        out = xtts_model.inference(\n",
        "            text=text,\n",
        "            language=language,\n",
        "            gpt_cond_latent=gpt_cond_latent,\n",
        "            speaker_embedding=speaker_embedding,\n",
        "            temperature=0.7,        # Param√®tre standard pour la cr√©ativit√©\n",
        "            length_penalty=1.0,     # P√©nalit√© de longueur\n",
        "            repetition_penalty=2.0, # √âvite les b√©gaiements\n",
        "            top_k=50,\n",
        "            top_p=0.8,\n",
        "            enable_text_splitting=enable_text_splitting\n",
        "        )\n",
        "\n",
        "        # Le r√©sultat est g√©n√©ralement dans un dictionnaire sous la cl√© 'wav'\n",
        "        if isinstance(out, dict) and 'wav' in out:\n",
        "            wav = out['wav']\n",
        "        else:\n",
        "            wav = out\n",
        "\n",
        "        # S'assurer que c'est bien un numpy array sur CPU\n",
        "        if hasattr(wav, 'cpu'):\n",
        "            wav = wav.cpu().numpy()\n",
        "        if isinstance(wav, list):\n",
        "            wav = np.array(wav, dtype=np.float32)\n",
        "\n",
        "        return wav\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Erreur lors de l'inf√©rence directe : {e}\")\n",
        "        raise e\n",
        "\n",
        "\n",
        "def text_to_speech_long(\n",
        "    text: str,\n",
        "    voice: str = \"female_fr\",\n",
        "    language: str = \"fr\",\n",
        "    output_path: Optional[str] = None,\n",
        "    enhance: bool = False,\n",
        "    use_gdrive: bool = False,\n",
        "    gdrive_folder: str = None,\n",
        "    max_chars_per_chunk: int = None,\n",
        "    show_progress: bool = True,\n",
        "    enable_text_splitting: bool = True\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    G√©n√®re un fichier audio long (> 1 heure) √† partir de texte.\n",
        "    \"\"\"\n",
        "    import torch\n",
        "\n",
        "    # Configuration\n",
        "    max_chars = max_chars_per_chunk or Config.MAX_CHARS_PER_CHUNK\n",
        "    voice_path = get_voice_path(voice)\n",
        "\n",
        "    # Estimation initiale\n",
        "    estimated_duration = TextSplitter.estimate_audio_duration(text)\n",
        "    print(f\"\\nüìù Texte: {len(text):,} caract√®res\")\n",
        "    print(f\"‚è±Ô∏è  Dur√©e estim√©e: {ProgressTracker._format_time(estimated_duration)}\")\n",
        "\n",
        "    # D√©couper le texte\n",
        "    chunks = TextSplitter.split_for_long_audio(text, max_chars=max_chars)\n",
        "    print(f\"üì¶ Chunks: {len(chunks)}\")\n",
        "\n",
        "    # Initialiser la progression\n",
        "    progress = None\n",
        "    if show_progress:\n",
        "        progress = ProgressTracker(len(chunks), \"üéôÔ∏è Synth√®se\")\n",
        "\n",
        "    # G√©n√©rer l'audio chunk par chunk\n",
        "    audio_chunks = []\n",
        "\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        chunk_start = time.time()\n",
        "\n",
        "        try:\n",
        "            wav = synthesize_chunk(\n",
        "                text=chunk,\n",
        "                voice_path=voice_path,\n",
        "                language=language,\n",
        "                enable_text_splitting=enable_text_splitting\n",
        "            )\n",
        "            audio_chunks.append(wav)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ö†Ô∏è Erreur chunk {i+1}: {e}\")\n",
        "            # Continuer avec les autres chunks\n",
        "            continue\n",
        "\n",
        "        # Lib√©rer la m√©moire GPU p√©riodiquement\n",
        "        if _device_name.startswith(\"cuda\") and (i + 1) % 10 == 0:\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        chunk_duration = time.time() - chunk_start\n",
        "        if progress:\n",
        "            progress.update(chunk_duration)\n",
        "\n",
        "    if not audio_chunks:\n",
        "        raise RuntimeError(\"Aucun audio g√©n√©r√©\")\n",
        "\n",
        "    print(\"\\nüîó Concat√©nation des chunks...\")\n",
        "\n",
        "    # Concat√©ner avec crossfade\n",
        "    final_audio = AudioProcessor.concatenate_chunks(\n",
        "        audio_chunks,\n",
        "        Config.SAMPLE_RATE,\n",
        "        Config.CROSSFADE_DURATION\n",
        "    )\n",
        "\n",
        "    # Lib√©rer les chunks de la m√©moire\n",
        "    del audio_chunks\n",
        "    gc.collect()\n",
        "    if _device_name.startswith(\"cuda\"):\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Post-traitement\n",
        "    if enhance:\n",
        "        print(\"‚ú® Post-traitement...\")\n",
        "        final_audio = AudioProcessor.enhance(\n",
        "            final_audio,\n",
        "            Config.SAMPLE_RATE,\n",
        "            normalize=True,\n",
        "            warmth=True\n",
        "        )\n",
        "    else:\n",
        "        final_audio = AudioProcessor.normalize(final_audio)\n",
        "\n",
        "    # Convertir en int16\n",
        "    final_audio = (final_audio * 32767).astype(np.int16)\n",
        "\n",
        "    # G√©n√©rer le nom de fichier\n",
        "    if output_path is None:\n",
        "        h = hashlib.md5(text[:100].encode()).hexdigest()[:8]\n",
        "        output_path = f\"tts_long_{voice}_{h}.wav\"\n",
        "\n",
        "    # Dossier de sortie\n",
        "    if use_gdrive:\n",
        "        folder = Path(gdrive_folder or Config.GDRIVE_FOLDER)\n",
        "        folder.mkdir(parents=True, exist_ok=True)\n",
        "        final_path = folder / Path(output_path).name\n",
        "    else:\n",
        "        final_path = Path(output_path)\n",
        "\n",
        "    # Sauvegarder\n",
        "    print(f\"üíæ Sauvegarde: {final_path}\")\n",
        "    with wave.open(str(final_path), \"wb\") as wav_file:\n",
        "        wav_file.setnchannels(1)\n",
        "        wav_file.setsampwidth(2)\n",
        "        wav_file.setframerate(Config.SAMPLE_RATE)\n",
        "        wav_file.writeframes(final_audio.tobytes())\n",
        "\n",
        "    # Calculer la dur√©e r√©elle\n",
        "    duration = len(final_audio) / Config.SAMPLE_RATE\n",
        "\n",
        "    print(f\"\\n‚úÖ Audio g√©n√©r√© avec succ√®s!\")\n",
        "    print(f\"   üìÅ Fichier: {final_path}\")\n",
        "    print(f\"   ‚è±Ô∏è  Dur√©e: {ProgressTracker._format_time(duration)}\")\n",
        "    print(f\"   üì¶ Chunks: {len(chunks)}\")\n",
        "    print(f\"   üé§ Voix: {voice}\")\n",
        "\n",
        "    return {\n",
        "        'path': str(final_path),\n",
        "        'sample_rate': Config.SAMPLE_RATE,\n",
        "        'duration_seconds': duration,\n",
        "        'duration_formatted': ProgressTracker._format_time(duration),\n",
        "        'audio_data': final_audio,\n",
        "        'voice': voice,\n",
        "        'language': language,\n",
        "        'device': _device_name,\n",
        "        'chunks_count': len(chunks),\n",
        "        'text_length': len(text)\n",
        "    }\n",
        "\n",
        "\n",
        "def text_to_speech(\n",
        "    text: str,\n",
        "    voice: str = \"female_fr\",\n",
        "    language: str = \"fr\",\n",
        "    output_path: Optional[str] = None,\n",
        "    enhance: bool = False,\n",
        "    use_gdrive: bool = False,\n",
        "    gdrive_folder: str = None,\n",
        "    enable_text_splitting: bool = True\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    G√©n√®re un fichier audio √† partir de texte avec XTTS v2.\n",
        "    \"\"\"\n",
        "    # Basculer automatiquement vers la version long pour textes > 10000 chars\n",
        "    if len(text) > 10000:\n",
        "        print(\"üì¢ Texte long d√©tect√© - utilisation de text_to_speech_long()\")\n",
        "        return text_to_speech_long(\n",
        "            text=text,\n",
        "            voice=voice,\n",
        "            language=language,\n",
        "            output_path=output_path,\n",
        "            enhance=enhance,\n",
        "            use_gdrive=use_gdrive,\n",
        "            gdrive_folder=gdrive_folder,\n",
        "            enable_text_splitting=enable_text_splitting\n",
        "        )\n",
        "\n",
        "    voice_path = get_voice_path(voice)\n",
        "\n",
        "    # G√©n√©rer l'audio avec enable_text_splitting\n",
        "    wav = synthesize_chunk(\n",
        "        text=text,\n",
        "        voice_path=voice_path,\n",
        "        language=language,\n",
        "        enable_text_splitting=enable_text_splitting\n",
        "    )\n",
        "\n",
        "    # Post-traitement\n",
        "    if enhance:\n",
        "        audio = AudioProcessor.enhance(wav, Config.SAMPLE_RATE)\n",
        "    else:\n",
        "        audio = AudioProcessor.normalize(wav)\n",
        "\n",
        "    audio = (audio * 32767).astype(np.int16)\n",
        "\n",
        "    # Nom de fichier\n",
        "    if output_path is None:\n",
        "        h = hashlib.md5(text.encode()).hexdigest()[:8]\n",
        "        output_path = f\"tts_{voice}_{h}.wav\"\n",
        "\n",
        "    # Dossier de sortie\n",
        "    if use_gdrive:\n",
        "        folder = Path(gdrive_folder or Config.GDRIVE_FOLDER)\n",
        "        folder.mkdir(parents=True, exist_ok=True)\n",
        "        final_path = folder / Path(output_path).name\n",
        "    else:\n",
        "        final_path = Path(output_path)\n",
        "\n",
        "    # Sauvegarder\n",
        "    with wave.open(str(final_path), \"wb\") as wav_file:\n",
        "        wav_file.setnchannels(1)\n",
        "        wav_file.setsampwidth(2)\n",
        "        wav_file.setframerate(Config.SAMPLE_RATE)\n",
        "        wav_file.writeframes(audio.tobytes())\n",
        "\n",
        "    duration = len(audio) / Config.SAMPLE_RATE\n",
        "\n",
        "    print(f\"‚úì Audio g√©n√©r√©: {final_path}\")\n",
        "    print(f\"  Dur√©e: {duration:.2f}s | Voix: {voice}\")\n",
        "\n",
        "    return {\n",
        "        'path': str(final_path),\n",
        "        'sample_rate': Config.SAMPLE_RATE,\n",
        "        'duration_seconds': duration,\n",
        "        'audio_data': audio,\n",
        "        'voice': voice,\n",
        "        'language': language,\n",
        "        'device': _device_name\n",
        "    }\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# UTILITIES\n",
        "# ==============================================================================\n",
        "\n",
        "def preview_audio(result: dict) -> None:\n",
        "    \"\"\"Pr√©visualise l'audio dans le notebook.\"\"\"\n",
        "    from IPython.display import Audio, display\n",
        "\n",
        "    audio = result['audio_data']\n",
        "    if audio.dtype == np.int16:\n",
        "        audio = audio.astype(np.float32) / 32768.0\n",
        "\n",
        "    display(Audio(audio, rate=result['sample_rate']))\n",
        "\n",
        "\n",
        "def list_voices() -> list:\n",
        "    \"\"\"Liste les voix disponibles.\"\"\"\n",
        "    return list(Config.PRESET_VOICES.keys())\n",
        "\n",
        "\n",
        "def list_languages() -> list:\n",
        "    \"\"\"Liste les langues support√©es.\"\"\"\n",
        "    return [\"en\", \"es\", \"fr\", \"de\", \"it\", \"pt\", \"pl\", \"tr\",\n",
        "            \"ru\", \"nl\", \"cs\", \"ar\", \"zh-cn\", \"ja\", \"hu\", \"ko\", \"hi\"]\n",
        "\n",
        "\n",
        "def clear_cache():\n",
        "    \"\"\"Lib√®re la m√©moire.\"\"\"\n",
        "    global _tts_model\n",
        "    import torch\n",
        "\n",
        "    _tts_model = None\n",
        "    gc.collect()\n",
        "\n",
        "    if _device_name.startswith(\"cuda\"):\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"‚úì Cache vid√©\")\n",
        "\n",
        "\n",
        "def estimate_duration(text: str) -> dict:\n",
        "    \"\"\"\n",
        "    Estime la dur√©e audio pour un texte.\n",
        "    \"\"\"\n",
        "    duration = TextSplitter.estimate_audio_duration(text)\n",
        "    chunks = len(TextSplitter.split_for_long_audio(text))\n",
        "\n",
        "    return {\n",
        "        'chars': len(text),\n",
        "        'estimated_seconds': duration,\n",
        "        'estimated_formatted': ProgressTracker._format_time(duration),\n",
        "        'chunks_estimate': chunks\n",
        "    }\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# ALIASES\n",
        "# ==============================================================================\n",
        "\n",
        "tts = text_to_speech\n",
        "tts_long = text_to_speech_long\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# INITIALIZATION\n",
        "# ==============================================================================\n",
        "\n",
        "def init():\n",
        "    \"\"\"Initialise le module.\"\"\"\n",
        "    detect_device()\n",
        "    print(\"‚úÖ Module XTTS v2 Long Audio charg√©\")\n",
        "    print(f\"   Device: {_device_name}\")\n",
        "    print(f\"   Voix: {list_voices()}\")\n",
        "    print(f\"   enable_text_splitting: activ√© par d√©faut\")\n",
        "\n",
        "\n",
        "# Auto-init\n",
        "if __name__ != \"__main__\":\n",
        "    try:\n",
        "        detect_device()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# EXAMPLE USAGE\n",
        "# ==============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Installation si n√©cessaire\n",
        "    install_dependencies()\n",
        "\n",
        "    # Initialisation\n",
        "    init()\n",
        "\n",
        "    # Exemple avec texte court\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"EXEMPLE 1: Texte court\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    short_text = \"\"\"\n",
        "    Ce document pr√©sente les Manifold-Constrained Hyper-Connections,\n",
        "    une architecture novatrice con√ßue par DeepSeek-AI pour stabiliser\n",
        "    l'entra√Ænement des grands mod√®les de langage.\n",
        "    \"\"\"\n",
        "\n",
        "    result = text_to_speech(\n",
        "        text=short_text.strip(),\n",
        "        voice=\"female_fr\",\n",
        "        enhance=True\n",
        "    )\n",
        "\n",
        "    print(f\"\\nR√©sultat: {result['duration_seconds']:.2f}s\")\n",
        "\n",
        "    # Exemple avec texte long (simul√©)\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"EXEMPLE 2: Estimation pour texte long\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Simuler un texte de ~1 heure (environ 54000 caract√®res)\n",
        "    long_text = short_text.strip() * 300  # ~54000 chars ‚âà 1 heure\n",
        "\n",
        "    estimation = estimate_duration(long_text)\n",
        "    print(f\"\\nEstimation pour {estimation['chars']:,} caract√®res:\")\n",
        "    print(f\"  Dur√©e: {estimation['estimated_formatted']}\")\n",
        "    print(f\"  Chunks: {estimation['chunks_estimate']}\")\n",
        "\n",
        "    # Pour g√©n√©rer r√©ellement:\n",
        "    # result = text_to_speech_long(long_text, voice=\"female_fr\", show_progress=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FQhKQ1IE4iX",
        "outputId": "f0b2eb8b-e071-40b0-e342-1395217fe769"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚öôÔ∏è Device: cuda (Tesla T4)\n",
            "‚úÖ Module XTTS v2 Long Audio charg√©\n",
            "   Device: cuda (Tesla T4)\n",
            "   Voix: ['female_fr', 'male_fr']\n",
            "   enable_text_splitting: activ√© par d√©faut\n",
            "\n",
            "============================================================\n",
            "EXEMPLE 1: Texte court\n",
            "============================================================\n",
            "üîÑ Chargement du mod√®le XTTS v2...\n",
            "‚úì Mod√®le charg√©\n",
            "‚úì Audio g√©n√©r√©: tts_female_fr_151473ed.wav\n",
            "  Dur√©e: 9.79s | Voix: female_fr\n",
            "\n",
            "R√©sultat: 9.79s\n",
            "\n",
            "============================================================\n",
            "EXEMPLE 2: Estimation pour texte long\n",
            "============================================================\n",
            "\n",
            "Estimation pour 55,200 caract√®res:\n",
            "  Dur√©e: 01:01:20\n",
            "  Chunks: 108\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lire le fichier\n",
        "with open(\"mon_texte_long.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    texte_complet = f.read()\n",
        "\n",
        "# Lancer la g√©n√©ration\n",
        "text_to_speech_long(\n",
        "    text=texte_complet,\n",
        "    voice=\"female_fr\",\n",
        "    language=\"fr\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRhVnXgSE7Yd",
        "outputId": "ce14c47a-70f3-4158-954b-9f5a415724f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìù Texte: 6,715 caract√®res\n",
            "‚è±Ô∏è  Dur√©e estim√©e: 07:27\n",
            "üì¶ Chunks: 16\n",
            "üéôÔ∏è Synth√®se [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 16/16 (100.0%) | Temps: 03:10 | ETA: 00:00\n",
            "\n",
            "üîó Concat√©nation des chunks...\n",
            "üíæ Sauvegarde: tts_long_female_fr_8aba435b.wav\n",
            "\n",
            "‚úÖ Audio g√©n√©r√© avec succ√®s!\n",
            "   üìÅ Fichier: tts_long_female_fr_8aba435b.wav\n",
            "   ‚è±Ô∏è  Dur√©e: 06:37\n",
            "   üì¶ Chunks: 16\n",
            "   üé§ Voix: female_fr\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'path': 'tts_long_female_fr_8aba435b.wav',\n",
              " 'sample_rate': 24000,\n",
              " 'duration_seconds': 397.586,\n",
              " 'duration_formatted': '06:37',\n",
              " 'audio_data': array([28, 21, 31, ...,  3,  7,  1], dtype=int16),\n",
              " 'voice': 'female_fr',\n",
              " 'language': 'fr',\n",
              " 'device': 'cuda (Tesla T4)',\n",
              " 'chunks_count': 16,\n",
              " 'text_length': 6715}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KaWW0-DIMy7R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}