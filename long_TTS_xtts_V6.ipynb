{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brunombo/Python/blob/master/long_TTS_xtts_V6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# TTS XTTS v2 - Long Audio Generator v6\n",
        "\n",
        "**Version 4.0** - Compatible PyTorch 2.9+ (Colab 2026)\n",
        "\n",
        "Fonctionnalites:\n",
        "- Generation audio longue duree (> 1 heure)\n",
        "- Fix torchcodec/torchaudio pour PyTorch 2.9+\n",
        "- Chunking intelligent par paragraphes\n",
        "- Crossfade entre chunks\n",
        "- Barre de progression avec ETA\n",
        "- Support Google Drive\n",
        "\n",
        "**Auteur:** Bruno | **Corrections:** Gemini, Claude"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "install_deps",
        "outputId": "6de5cc81-b222-406b-8ceb-31d525486be5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch = 2.9.0+cu128 -> installation de torchcodec==0.9.* via https://download.pytorch.org/whl/cu128\n",
            "torchcodec detecte: True\n",
            "torchcodec version: 0.9.1+cu128\n"
          ]
        }
      ],
      "source": [
        "# Installation des dependances\n",
        "# --------------------------------------------------------------\n",
        "# Remarque importante (PyTorch>=2.9) :\n",
        "# - Coqui TTS exige la bibliotheque `torchcodec` pour l'I/O audio.  (cf. message d'erreur)\n",
        "# - La version de torchcodec doit etre compatible avec votre version de torch.\n",
        "#\n",
        "# Sources (documentation officielle) :\n",
        "# - coqui-tts: installer torch, torchaudio et (seulement pour torch>=2.9) torchcodec.\n",
        "# - torchcodec: table de compatibilite torch <-> torchcodec + note CUDA/CPU.\n",
        "\n",
        "!pip install -q -U pip\n",
        "!pip install -q numpy==2.0.2 scipy soundfile noisereduce\n",
        "!pip install -q -U coqui-tts\n",
        "\n",
        "\n",
        "# Installer torchcodec dans une version compatible avec torch (et CUDA si detecte)\n",
        "import sys, subprocess, re\n",
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\n",
        "        \"PyTorch (torch) n'est pas importable. Installez d'abord torch/torchaudio, \"\n",
        "        \"puis relancez cette cellule.\"\n",
        "    ) from e\n",
        "\n",
        "\n",
        "def _torch_major_minor(ver: str) -> str:\n",
        "    base = ver.split(\"+\")[0]\n",
        "    parts = base.split(\".\")\n",
        "    return \".\".join(parts[:2]) if len(parts) >= 2 else base\n",
        "\n",
        "\n",
        "torch_ver = torch.__version__\n",
        "mm = _torch_major_minor(torch_ver)\n",
        "\n",
        "# Mapping base sur la table de compatibilite officielle torchcodec.\n",
        "if mm == \"2.10\":\n",
        "    torchcodec_spec = \"torchcodec==0.10.*\"\n",
        "elif mm == \"2.9\":\n",
        "    torchcodec_spec = \"torchcodec==0.9.*\"\n",
        "elif mm == \"2.8\":\n",
        "    torchcodec_spec = \"torchcodec==0.7.*\"\n",
        "else:\n",
        "    torchcodec_spec = \"torchcodec\"\n",
        "\n",
        "# Si votre torch est un build CUDA (ex: 2.9.0+cu126), on tente d'installer torchcodec\n",
        "# depuis l'index PyTorch correspondant. Sinon, on installe la version CPU depuis PyPI.\n",
        "index_url = None\n",
        "if \"+\" in torch_ver:\n",
        "    build = torch_ver.split(\"+\", 1)[1]\n",
        "    if build.startswith(\"cu\"):\n",
        "        index_url = f\"https://download.pytorch.org/whl/{build}\"\n",
        "\n",
        "print(\n",
        "    f\"torch = {torch_ver} -> installation de {torchcodec_spec}\"\n",
        "    + (f\" via {index_url}\" if index_url else \" (CPU PyPI)\")\n",
        ")\n",
        "\n",
        "\n",
        "def _pip_install_torchcodec():\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-U\", torchcodec_spec]\n",
        "    if index_url:\n",
        "        cmd += [\"--index-url\", index_url]\n",
        "    subprocess.check_call(cmd)\n",
        "\n",
        "\n",
        "try:\n",
        "    _pip_install_torchcodec()\n",
        "except Exception as e:\n",
        "    # Fallback : essayer sans index_url (CPU PyPI).\n",
        "    if index_url:\n",
        "        print(f\"âš ï¸ Echec avec l'index PyTorch ({index_url}). Tentative CPU via PyPIâ€¦\")\n",
        "        index_url = None\n",
        "        _pip_install_torchcodec()\n",
        "    else:\n",
        "        raise\n",
        "\n",
        "# Verification (metadonnees pip)\n",
        "import importlib.util, importlib.metadata\n",
        "\n",
        "print(\"torchcodec detecte:\", importlib.util.find_spec(\"torchcodec\") is not None)\n",
        "print(\"torchcodec version:\", importlib.metadata.version(\"torchcodec\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "main_module",
        "outputId": "be31dd77-c6f6-4e51-ae1e-c982111a0300"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Patch torchaudio applique (backend: soundfile)\n",
            "âš™ï¸ Device: cuda (Tesla T4)\n",
            "\n",
            "============================================================\n",
            "TTS XTTS v2 - Long Audio Generator v4\n",
            "Compatible PyTorch 2.9+ (fix torchcodec)\n",
            "============================================================\n",
            "Voix disponibles: ['female_fr', 'male_fr']\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "TTS XTTS v2 - Version Long Audio v4\n",
        "====================================\n",
        "\n",
        "Module de synthese vocale haute qualite utilisant Coqui XTTS v2.\n",
        "Compatible avec PyTorch 2.9+ (fix torchcodec/torchaudio).\n",
        "\n",
        "Auteur: Bruno\n",
        "Date: Janvier 2026\n",
        "Corrections: Gemini, Claude\n",
        "\"\"\"\n",
        "\n",
        "# ==============================================================================\n",
        "# IMPORTS STANDARDS (APRES LE FIX)\n",
        "# ==============================================================================\n",
        "\n",
        "import os\n",
        "import re\n",
        "import gc\n",
        "import wave\n",
        "import time\n",
        "import hashlib\n",
        "import warnings\n",
        "import inspect\n",
        "from pathlib import Path\n",
        "from typing import Optional, List\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# ==============================================================================\n",
        "# TORCHAUDIO FIX - Backend soundfile\n",
        "# ==============================================================================\n",
        "\n",
        "\n",
        "def _patch_torchaudio():\n",
        "    \"\"\"\n",
        "    Patch torchaudio.load pour utiliser le backend soundfile au lieu de torchcodec.\n",
        "    Resout l'erreur: \"Could not load libtorchcodec\" sur Colab avec PyTorch 2.9+.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import torchaudio\n",
        "\n",
        "        # Verifier si deja patche\n",
        "        if hasattr(torchaudio, \"_original_load_patched\"):\n",
        "            return\n",
        "\n",
        "        # Sauvegarder la fonction originale\n",
        "        _original_load = torchaudio.load\n",
        "\n",
        "        def _patched_load(filepath, *args, **kwargs):\n",
        "            \"\"\"\n",
        "            Version patchee de torchaudio.load qui utilise soundfile comme backend.\n",
        "            \"\"\"\n",
        "            # Forcer le backend soundfile si non specifie\n",
        "            if \"backend\" not in kwargs:\n",
        "                kwargs[\"backend\"] = \"soundfile\"\n",
        "\n",
        "            try:\n",
        "                return _original_load(filepath, *args, **kwargs)\n",
        "            except Exception as e:\n",
        "                # Si soundfile echoue, essayer sans specifier de backend\n",
        "                if \"backend\" in kwargs:\n",
        "                    del kwargs[\"backend\"]\n",
        "                    try:\n",
        "                        return _original_load(filepath, *args, **kwargs)\n",
        "                    except:\n",
        "                        pass\n",
        "                raise e\n",
        "\n",
        "        # Appliquer le patch\n",
        "        torchaudio.load = _patched_load\n",
        "        torchaudio._original_load_patched = True\n",
        "        print(\"âœ“ Patch torchaudio applique (backend: soundfile)\")\n",
        "\n",
        "    except ImportError:\n",
        "        pass\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Impossible de patcher torchaudio: {e}\")\n",
        "\n",
        "\n",
        "# Appliquer le patch torchaudio\n",
        "_patch_torchaudio()\n",
        "\n",
        "# ==============================================================================\n",
        "# CONFIGURATION\n",
        "# ==============================================================================\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TTSConfig:\n",
        "    \"\"\"Configuration globale du module TTS.\"\"\"\n",
        "\n",
        "    MODEL_NAME: str = \"tts_models/multilingual/multi-dataset/xtts_v2\"\n",
        "    SAMPLE_RATE: int = 24000\n",
        "    DEFAULT_LANGUAGE: str = \"fr\"\n",
        "    GDRIVE_FOLDER: str = \"/content/drive/MyDrive/TTS_Output\"\n",
        "    MAX_CHARS_PER_CHUNK: int = 500\n",
        "    CROSSFADE_DURATION: float = 0.05\n",
        "    ENABLE_TEXT_SPLITTING: bool = True\n",
        "    PRESET_VOICES: dict = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.PRESET_VOICES = {\n",
        "            \"female_fr\": \"https://huggingface.co/spaces/coqui/xtts/resolve/main/examples/female.wav\",\n",
        "            \"male_fr\": \"https://huggingface.co/spaces/coqui/xtts/resolve/main/examples/male.wav\",\n",
        "        }\n",
        "\n",
        "\n",
        "Config = TTSConfig()\n",
        "\n",
        "# ==============================================================================\n",
        "# DEVICE MANAGEMENT\n",
        "# ==============================================================================\n",
        "\n",
        "_device = None\n",
        "_device_name = \"cpu\"\n",
        "\n",
        "\n",
        "def detect_device():\n",
        "    \"\"\"Detecte le meilleur device disponible.\"\"\"\n",
        "    global _device, _device_name\n",
        "    import torch\n",
        "\n",
        "    # Essayer TPU\n",
        "    try:\n",
        "        import torch_xla.core.xla_model as xm\n",
        "\n",
        "        _device = xm.xla_device()\n",
        "        _device_name = \"tpu\"\n",
        "        print(f\"âš™ï¸ Device: TPU\")\n",
        "        return\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Essayer CUDA\n",
        "    if torch.cuda.is_available():\n",
        "        _device = torch.device(\"cuda\")\n",
        "        _device_name = f\"cuda ({torch.cuda.get_device_name(0)})\"\n",
        "        print(f\"âš™ï¸ Device: {_device_name}\")\n",
        "        return\n",
        "\n",
        "    # Fallback CPU\n",
        "    _device = torch.device(\"cpu\")\n",
        "    _device_name = \"cpu\"\n",
        "    print(f\"âš™ï¸ Device: CPU\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# TEXT SPLITTING UTILITIES\n",
        "# ==============================================================================\n",
        "\n",
        "\n",
        "class TextSplitter:\n",
        "    \"\"\"Utilitaire pour decouper intelligemment les textes longs.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def estimate_audio_duration(text: str, chars_per_second: float = 15.0) -> float:\n",
        "        \"\"\"Estime la duree audio en secondes.\"\"\"\n",
        "        return len(text) / chars_per_second\n",
        "\n",
        "    @staticmethod\n",
        "    def split_into_sentences(text: str) -> List[str]:\n",
        "        \"\"\"Decoupe le texte en phrases.\"\"\"\n",
        "        pattern = r\"(?<=[.!?])\\s+\"\n",
        "        sentences = re.split(pattern, text)\n",
        "        return [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "    @staticmethod\n",
        "    def split_into_paragraphs(text: str) -> List[str]:\n",
        "        \"\"\"Decoupe le texte en paragraphes.\"\"\"\n",
        "        paragraphs = re.split(r\"\\n\\s*\\n\", text)\n",
        "        return [p.strip() for p in paragraphs if p.strip()]\n",
        "\n",
        "    @classmethod\n",
        "    def split_for_long_audio(\n",
        "        cls, text: str, max_chars: int = 500, preserve_sentences: bool = True\n",
        "    ) -> List[str]:\n",
        "        \"\"\"Decoupe le texte pour generation audio longue.\"\"\"\n",
        "        if len(text) <= max_chars:\n",
        "            return [text]\n",
        "\n",
        "        chunks = []\n",
        "        if preserve_sentences:\n",
        "            sentences = cls.split_into_sentences(text)\n",
        "            current_chunk = \"\"\n",
        "\n",
        "            for sentence in sentences:\n",
        "                if len(sentence) > max_chars:\n",
        "                    if current_chunk:\n",
        "                        chunks.append(current_chunk.strip())\n",
        "                        current_chunk = \"\"\n",
        "                    # Decouper la phrase trop longue par mots\n",
        "                    words = sentence.split()\n",
        "                    sub_chunk = \"\"\n",
        "                    for word in words:\n",
        "                        if len(sub_chunk) + len(word) + 1 <= max_chars:\n",
        "                            sub_chunk += \" \" + word if sub_chunk else word\n",
        "                        else:\n",
        "                            if sub_chunk:\n",
        "                                chunks.append(sub_chunk.strip())\n",
        "                            sub_chunk = word\n",
        "                    if sub_chunk:\n",
        "                        current_chunk = sub_chunk\n",
        "                elif len(current_chunk) + len(sentence) + 1 <= max_chars:\n",
        "                    current_chunk += \" \" + sentence if current_chunk else sentence\n",
        "                else:\n",
        "                    if current_chunk:\n",
        "                        chunks.append(current_chunk.strip())\n",
        "                    current_chunk = sentence\n",
        "\n",
        "            if current_chunk:\n",
        "                chunks.append(current_chunk.strip())\n",
        "        else:\n",
        "            for i in range(0, len(text), max_chars):\n",
        "                chunks.append(text[i : i + max_chars])\n",
        "\n",
        "        return chunks\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# AUDIO PROCESSING\n",
        "# ==============================================================================\n",
        "\n",
        "\n",
        "class AudioProcessor:\n",
        "    \"\"\"Processeur audio pour post-traitement et concatenation.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize(audio: np.ndarray, target_db: float = -3.0) -> np.ndarray:\n",
        "        \"\"\"Normalise l'audio au niveau cible.\"\"\"\n",
        "        if audio.dtype == np.int16:\n",
        "            audio = audio.astype(np.float32) / 32768.0\n",
        "        peak = np.max(np.abs(audio))\n",
        "        if peak > 0:\n",
        "            target_linear = 10 ** (target_db / 20)\n",
        "            audio = audio * (target_linear / peak)\n",
        "        return np.clip(audio, -1.0, 1.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def crossfade(\n",
        "        audio1: np.ndarray, audio2: np.ndarray, sample_rate: int, duration: float = 0.05\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"Concatene deux segments audio avec crossfade.\"\"\"\n",
        "        if audio1.dtype == np.int16:\n",
        "            audio1 = audio1.astype(np.float32) / 32768.0\n",
        "        if audio2.dtype == np.int16:\n",
        "            audio2 = audio2.astype(np.float32) / 32768.0\n",
        "\n",
        "        fade_samples = int(sample_rate * duration)\n",
        "        if len(audio1) < fade_samples or len(audio2) < fade_samples:\n",
        "            return np.concatenate([audio1, audio2])\n",
        "\n",
        "        fade_out = np.linspace(1.0, 0.0, fade_samples)\n",
        "        fade_in = np.linspace(0.0, 1.0, fade_samples)\n",
        "        audio1_end = audio1[-fade_samples:] * fade_out\n",
        "        audio2_start = audio2[:fade_samples] * fade_in\n",
        "\n",
        "        return np.concatenate(\n",
        "            [audio1[:-fade_samples], audio1_end + audio2_start, audio2[fade_samples:]]\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def concatenate_chunks(\n",
        "        cls,\n",
        "        audio_chunks: List[np.ndarray],\n",
        "        sample_rate: int,\n",
        "        crossfade_duration: float = 0.05,\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"Concatene plusieurs chunks audio avec crossfade.\"\"\"\n",
        "        if not audio_chunks:\n",
        "            return np.array([], dtype=np.float32)\n",
        "        if len(audio_chunks) == 1:\n",
        "            audio = audio_chunks[0]\n",
        "            if audio.dtype == np.int16:\n",
        "                audio = audio.astype(np.float32) / 32768.0\n",
        "            return audio\n",
        "\n",
        "        result = audio_chunks[0]\n",
        "        if result.dtype == np.int16:\n",
        "            result = result.astype(np.float32) / 32768.0\n",
        "        for chunk in audio_chunks[1:]:\n",
        "            result = cls.crossfade(result, chunk, sample_rate, crossfade_duration)\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def enhance(\n",
        "        audio: np.ndarray, sample_rate: int, normalize: bool = True, warmth: bool = True\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"Ameliore l'audio avec normalisation et warmth.\"\"\"\n",
        "        if audio.dtype == np.int16:\n",
        "            audio = audio.astype(np.float32) / 32768.0\n",
        "\n",
        "        # Ajouter de la chaleur (boost basses frequences)\n",
        "        if warmth:\n",
        "            try:\n",
        "                from scipy import signal\n",
        "\n",
        "                nyquist = sample_rate / 2\n",
        "                cutoff = min(300, nyquist * 0.9) / nyquist\n",
        "                b, a = signal.butter(2, cutoff, btype=\"low\")\n",
        "                bass = signal.filtfilt(b, a, audio)\n",
        "                audio = audio + 0.15 * bass\n",
        "            except ImportError:\n",
        "                pass\n",
        "\n",
        "        # Normaliser\n",
        "        if normalize:\n",
        "            peak = np.max(np.abs(audio))\n",
        "            if peak > 0:\n",
        "                target = 10 ** (-3.0 / 20)\n",
        "                audio = audio * (target / peak)\n",
        "\n",
        "        return np.clip(audio, -1.0, 1.0)\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# PROGRESS TRACKER\n",
        "# ==============================================================================\n",
        "\n",
        "\n",
        "class ProgressTracker:\n",
        "    \"\"\"Suivi de progression avec estimation du temps restant.\"\"\"\n",
        "\n",
        "    def __init__(self, total: int, description: str = \"\"):\n",
        "        self.total = total\n",
        "        self.current = 0\n",
        "        self.description = description\n",
        "        self.start_time = time.time()\n",
        "        self.chunk_times = []\n",
        "\n",
        "    def update(self, chunk_duration: float = None):\n",
        "        \"\"\"Met a jour la progression.\"\"\"\n",
        "        self.current += 1\n",
        "        if chunk_duration:\n",
        "            self.chunk_times.append(chunk_duration)\n",
        "        self._display()\n",
        "\n",
        "    def _display(self):\n",
        "        \"\"\"Affiche la barre de progression.\"\"\"\n",
        "        elapsed = time.time() - self.start_time\n",
        "        percent = (self.current / self.total) * 100\n",
        "\n",
        "        if self.chunk_times:\n",
        "            avg_time = np.mean(self.chunk_times)\n",
        "            remaining = avg_time * (self.total - self.current)\n",
        "            eta_str = self._format_time(remaining)\n",
        "        else:\n",
        "            eta_str = \"...\"\n",
        "\n",
        "        bar_length = 30\n",
        "        filled = int(bar_length * self.current / self.total)\n",
        "        bar = \"â–ˆ\" * filled + \"â–‘\" * (bar_length - filled)\n",
        "        elapsed_str = self._format_time(elapsed)\n",
        "\n",
        "        print(\n",
        "            f\"\\r{self.description} [{bar}] {self.current}/{self.total} \"\n",
        "            f\"({percent:.1f}%) | Temps: {elapsed_str} | ETA: {eta_str}\",\n",
        "            end=\"\",\n",
        "        )\n",
        "\n",
        "        if self.current >= self.total:\n",
        "            print()\n",
        "\n",
        "    @staticmethod\n",
        "    def _format_time(seconds: float) -> str:\n",
        "        \"\"\"Formate les secondes en HH:MM:SS.\"\"\"\n",
        "        hours = int(seconds // 3600)\n",
        "        minutes = int((seconds % 3600) // 60)\n",
        "        secs = int(seconds % 60)\n",
        "        if hours > 0:\n",
        "            return f\"{hours:02d}:{minutes:02d}:{secs:02d}\"\n",
        "        return f\"{minutes:02d}:{secs:02d}\"\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# TTS ENGINE\n",
        "# ==============================================================================\n",
        "\n",
        "_tts_model = None\n",
        "_voices_cache = {}\n",
        "os.environ[\"COQUI_TOS_AGREED\"] = \"1\"\n",
        "\n",
        "\n",
        "def get_model():\n",
        "    \"\"\"Charge le modele XTTS v2 avec cache.\"\"\"\n",
        "    global _tts_model\n",
        "\n",
        "    if _tts_model is None:\n",
        "        print(\"ðŸ”„ Chargement du modele XTTS v2...\")\n",
        "\n",
        "        from TTS.api import TTS\n",
        "\n",
        "        _tts_model = TTS(Config.MODEL_NAME)\n",
        "\n",
        "        # Deplacement sur GPU (selon la version, .to() peut etre sur le wrapper ou sur le sous-modele)\n",
        "        if _device is not None and _device_name.startswith(\"cuda\"):\n",
        "            try:\n",
        "                if hasattr(_tts_model, \"to\"):\n",
        "                    _tts_model = _tts_model.to(_device)\n",
        "                elif hasattr(_tts_model, \"tts_model\") and hasattr(\n",
        "                    _tts_model.tts_model, \"to\"\n",
        "                ):\n",
        "                    _tts_model.tts_model = _tts_model.tts_model.to(_device)\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ Impossible de deplacer le modele sur CUDA: {e}\")\n",
        "\n",
        "        print(\"âœ“ Modele charge\")\n",
        "\n",
        "    return _tts_model\n",
        "\n",
        "\n",
        "def get_voice_path(voice: str) -> str:\n",
        "    \"\"\"Obtient le chemin vers un fichier de voix.\"\"\"\n",
        "    global _voices_cache\n",
        "    import urllib.request\n",
        "\n",
        "    if voice in _voices_cache:\n",
        "        return _voices_cache[voice]\n",
        "\n",
        "    if os.path.isfile(voice):\n",
        "        _voices_cache[voice] = voice\n",
        "        return voice\n",
        "\n",
        "    if voice in Config.PRESET_VOICES:\n",
        "        url = Config.PRESET_VOICES[voice]\n",
        "        path = f\"/tmp/{voice}.wav\"\n",
        "        if not os.path.exists(path):\n",
        "            print(f\"ðŸ“¥ Telechargement de la voix '{voice}'...\")\n",
        "            urllib.request.urlretrieve(url, path)\n",
        "        _voices_cache[voice] = path\n",
        "        return path\n",
        "\n",
        "    raise FileNotFoundError(f\"Voix '{voice}' non trouvee\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# MAIN SYNTHESIS FUNCTIONS\n",
        "# ==============================================================================\n",
        "\n",
        "\n",
        "def _filter_kwargs(fn, kwargs: dict) -> dict:\n",
        "    \"\"\"Garde uniquement les kwargs acceptes par fn (compatibilite entre versions).\"\"\"\n",
        "    try:\n",
        "        sig = inspect.signature(fn)\n",
        "        return {k: v for k, v in kwargs.items() if k in sig.parameters}\n",
        "    except (TypeError, ValueError):\n",
        "        # Signature indisponible (ex: fonction C++) -> on ne filtre pas\n",
        "        return kwargs\n",
        "\n",
        "\n",
        "def _get_conditioning_latents_compat(xtts_model, voice_path: str):\n",
        "    \"\"\"Compat: get_conditioning_latents() a change de signature selon les versions.\"\"\"\n",
        "    fn = getattr(xtts_model, \"get_conditioning_latents\", None)\n",
        "    if fn is None:\n",
        "        raise AttributeError(\n",
        "            \"Le modele XTTS ne fournit pas get_conditioning_latents().\"\n",
        "        )\n",
        "\n",
        "    base_kwargs = {\"gpt_cond_len\": 30, \"max_ref_length\": 60}\n",
        "\n",
        "    # Tentative par introspection\n",
        "    try:\n",
        "        sig = inspect.signature(fn)\n",
        "        params = sig.parameters\n",
        "\n",
        "        if \"audio_path\" in params:\n",
        "            # Certaines versions veulent une liste, d'autres une str\n",
        "            try:\n",
        "                return fn(audio_path=[voice_path], **_filter_kwargs(fn, base_kwargs))\n",
        "            except TypeError:\n",
        "                return fn(audio_path=voice_path, **_filter_kwargs(fn, base_kwargs))\n",
        "\n",
        "        if \"audio_paths\" in params:\n",
        "            return fn(audio_paths=[voice_path], **_filter_kwargs(fn, base_kwargs))\n",
        "\n",
        "        if \"speaker_wav\" in params:\n",
        "            return fn(speaker_wav=voice_path, **_filter_kwargs(fn, base_kwargs))\n",
        "\n",
        "    except (TypeError, ValueError):\n",
        "        pass\n",
        "\n",
        "    # Fallback brut (plus permissif)\n",
        "    try:\n",
        "        return fn(audio_path=[voice_path], gpt_cond_len=30, max_ref_length=60)\n",
        "    except Exception:\n",
        "        try:\n",
        "            return fn(audio_path=voice_path, gpt_cond_len=30, max_ref_length=60)\n",
        "        except Exception:\n",
        "            return fn(voice_path)\n",
        "\n",
        "\n",
        "def synthesize_chunk(\n",
        "    text: str, voice_path: str, language: str = \"fr\", enable_text_splitting: bool = True\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Synthetise un chunk de texte en audio via l'inference directe.\"\"\"\n",
        "    model_wrapper = get_model()\n",
        "\n",
        "    # Acceder au modele XTTS directement (bypass SpeakerManager bug)\n",
        "    if hasattr(model_wrapper, \"synthesizer\"):\n",
        "        xtts_model = model_wrapper.synthesizer.tts_model\n",
        "    else:\n",
        "        xtts_model = model_wrapper.tts_model\n",
        "\n",
        "    # Calculer les latents de conditionnement (compat multi-versions)\n",
        "    try:\n",
        "        gpt_cond_latent, speaker_embedding = _get_conditioning_latents_compat(\n",
        "            xtts_model, voice_path\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Erreur calcul latents: {e}\")\n",
        "        raise e\n",
        "\n",
        "    # Inference directe (filtrage des kwargs selon la signature)\n",
        "    try:\n",
        "        inference_kwargs = {\n",
        "            \"text\": text,\n",
        "            \"language\": language,\n",
        "            \"gpt_cond_latent\": gpt_cond_latent,\n",
        "            \"speaker_embedding\": speaker_embedding,\n",
        "            \"temperature\": 0.7,\n",
        "            \"length_penalty\": 1.0,\n",
        "            \"repetition_penalty\": 2.0,\n",
        "            \"top_k\": 50,\n",
        "            \"top_p\": 0.8,\n",
        "            \"enable_text_splitting\": enable_text_splitting,\n",
        "        }\n",
        "\n",
        "        # Alias possibles selon versions\n",
        "        try:\n",
        "            sig = inspect.signature(xtts_model.inference)\n",
        "            params = sig.parameters\n",
        "            if \"speaker_embedding\" not in params and \"speaker_latents\" in params:\n",
        "                inference_kwargs[\"speaker_latents\"] = inference_kwargs.pop(\n",
        "                    \"speaker_embedding\"\n",
        "                )\n",
        "        except (TypeError, ValueError):\n",
        "            pass\n",
        "\n",
        "        out = xtts_model.inference(\n",
        "            **_filter_kwargs(xtts_model.inference, inference_kwargs)\n",
        "        )\n",
        "\n",
        "        if isinstance(out, dict) and \"wav\" in out:\n",
        "            wav = out[\"wav\"]\n",
        "        else:\n",
        "            wav = out\n",
        "\n",
        "        if hasattr(wav, \"cpu\"):\n",
        "            wav = wav.cpu().numpy()\n",
        "        if isinstance(wav, list):\n",
        "            wav = np.array(wav, dtype=np.float32)\n",
        "\n",
        "        return wav\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Erreur lors de l'inference directe : {e}\")\n",
        "        raise e\n",
        "\n",
        "\n",
        "def text_to_speech_long(\n",
        "    text: str,\n",
        "    voice: str = \"female_fr\",\n",
        "    language: str = \"fr\",\n",
        "    output_path: Optional[str] = None,\n",
        "    enhance: bool = False,\n",
        "    use_gdrive: bool = False,\n",
        "    gdrive_folder: str = None,\n",
        "    max_chars_per_chunk: int = None,\n",
        "    show_progress: bool = True,\n",
        "    enable_text_splitting: bool = True,\n",
        ") -> dict:\n",
        "    \"\"\"Genere un fichier audio long (> 1 heure) a partir de texte.\"\"\"\n",
        "    import torch\n",
        "\n",
        "    max_chars = max_chars_per_chunk or Config.MAX_CHARS_PER_CHUNK\n",
        "    voice_path = get_voice_path(voice)\n",
        "\n",
        "    # Estimation\n",
        "    estimated_duration = TextSplitter.estimate_audio_duration(text)\n",
        "    print(f\"\\nðŸ“ Texte: {len(text):,} caracteres\")\n",
        "    print(f\"â±ï¸  Duree estimee: {ProgressTracker._format_time(estimated_duration)}\")\n",
        "\n",
        "    # Decoupage\n",
        "    chunks = TextSplitter.split_for_long_audio(text, max_chars=max_chars)\n",
        "    print(f\"ðŸ“¦ Chunks: {len(chunks)}\")\n",
        "\n",
        "    # Synthese\n",
        "    progress = ProgressTracker(len(chunks), \"ðŸŽ™ï¸ Synthese\") if show_progress else None\n",
        "    audio_chunks = []\n",
        "\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        chunk_start = time.time()\n",
        "        try:\n",
        "            wav = synthesize_chunk(\n",
        "                text=chunk,\n",
        "                voice_path=voice_path,\n",
        "                language=language,\n",
        "                enable_text_splitting=enable_text_splitting,\n",
        "            )\n",
        "            audio_chunks.append(wav)\n",
        "        except Exception as e:\n",
        "            print(f\"\\nâš ï¸ Erreur chunk {i + 1}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Nettoyage memoire periodique\n",
        "        if _device_name.startswith(\"cuda\") and (i + 1) % 10 == 0:\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        if progress:\n",
        "            progress.update(time.time() - chunk_start)\n",
        "\n",
        "    if not audio_chunks:\n",
        "        raise RuntimeError(\"Aucun audio genere\")\n",
        "\n",
        "    # Concatenation\n",
        "    print(\"\\nðŸ”— Concatenation des chunks...\")\n",
        "    final_audio = AudioProcessor.concatenate_chunks(\n",
        "        audio_chunks, Config.SAMPLE_RATE, Config.CROSSFADE_DURATION\n",
        "    )\n",
        "\n",
        "    # Nettoyage memoire\n",
        "    del audio_chunks\n",
        "    gc.collect()\n",
        "    if _device_name.startswith(\"cuda\"):\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Post-traitement\n",
        "    if enhance:\n",
        "        print(\"âœ¨ Post-traitement...\")\n",
        "        final_audio = AudioProcessor.enhance(\n",
        "            final_audio, Config.SAMPLE_RATE, normalize=True, warmth=True\n",
        "        )\n",
        "    else:\n",
        "        final_audio = AudioProcessor.normalize(final_audio)\n",
        "\n",
        "    # Conversion en int16\n",
        "    final_audio = (final_audio * 32767).astype(np.int16)\n",
        "\n",
        "    # Chemin de sortie\n",
        "    if output_path is None:\n",
        "        h = hashlib.md5(text[:100].encode()).hexdigest()[:8]\n",
        "        output_path = f\"tts_long_{voice}_{h}.wav\"\n",
        "\n",
        "    if use_gdrive:\n",
        "        folder = Path(gdrive_folder or Config.GDRIVE_FOLDER)\n",
        "        folder.mkdir(parents=True, exist_ok=True)\n",
        "        final_path = folder / Path(output_path).name\n",
        "    else:\n",
        "        final_path = Path(output_path)\n",
        "\n",
        "    # Sauvegarde WAV\n",
        "    print(f\"ðŸ’¾ Sauvegarde: {final_path}\")\n",
        "    with wave.open(str(final_path), \"wb\") as wav_file:\n",
        "        wav_file.setnchannels(1)\n",
        "        wav_file.setsampwidth(2)\n",
        "        wav_file.setframerate(Config.SAMPLE_RATE)\n",
        "        wav_file.writeframes(final_audio.tobytes())\n",
        "\n",
        "    duration = len(final_audio) / Config.SAMPLE_RATE\n",
        "\n",
        "    print(f\"\\nâœ… Audio genere avec succes!\")\n",
        "    print(f\"   ðŸ“ Fichier: {final_path}\")\n",
        "    print(f\"   â±ï¸  Duree: {ProgressTracker._format_time(duration)}\")\n",
        "    print(f\"   ðŸ“¦ Chunks: {len(chunks)}\")\n",
        "    print(f\"   ðŸŽ¤ Voix: {voice}\")\n",
        "\n",
        "    return {\n",
        "        \"path\": str(final_path),\n",
        "        \"sample_rate\": Config.SAMPLE_RATE,\n",
        "        \"duration_seconds\": duration,\n",
        "        \"duration_formatted\": ProgressTracker._format_time(duration),\n",
        "        \"audio_data\": final_audio,\n",
        "        \"voice\": voice,\n",
        "        \"language\": language,\n",
        "        \"device\": _device_name,\n",
        "        \"chunks_count\": len(chunks),\n",
        "        \"text_length\": len(text),\n",
        "    }\n",
        "\n",
        "\n",
        "def text_to_speech(\n",
        "    text: str,\n",
        "    voice: str = \"female_fr\",\n",
        "    language: str = \"fr\",\n",
        "    output_path: Optional[str] = None,\n",
        "    enhance: bool = False,\n",
        "    use_gdrive: bool = False,\n",
        "    gdrive_folder: str = None,\n",
        "    enable_text_splitting: bool = True,\n",
        ") -> dict:\n",
        "    \"\"\"Genere un fichier audio a partir de texte avec XTTS v2.\"\"\"\n",
        "    # Rediriger vers version longue si necessaire\n",
        "    if len(text) > 10000:\n",
        "        print(\"ðŸ“¢ Texte long detecte - utilisation de text_to_speech_long()\")\n",
        "        return text_to_speech_long(\n",
        "            text=text,\n",
        "            voice=voice,\n",
        "            language=language,\n",
        "            output_path=output_path,\n",
        "            enhance=enhance,\n",
        "            use_gdrive=use_gdrive,\n",
        "            gdrive_folder=gdrive_folder,\n",
        "            enable_text_splitting=enable_text_splitting,\n",
        "        )\n",
        "\n",
        "    voice_path = get_voice_path(voice)\n",
        "\n",
        "    # Synthese\n",
        "    wav = synthesize_chunk(\n",
        "        text=text,\n",
        "        voice_path=voice_path,\n",
        "        language=language,\n",
        "        enable_text_splitting=enable_text_splitting,\n",
        "    )\n",
        "\n",
        "    # Post-traitement\n",
        "    if enhance:\n",
        "        audio = AudioProcessor.enhance(wav, Config.SAMPLE_RATE)\n",
        "    else:\n",
        "        audio = AudioProcessor.normalize(wav)\n",
        "\n",
        "    audio = (audio * 32767).astype(np.int16)\n",
        "\n",
        "    # Chemin de sortie\n",
        "    if output_path is None:\n",
        "        h = hashlib.md5(text.encode()).hexdigest()[:8]\n",
        "        output_path = f\"tts_{voice}_{h}.wav\"\n",
        "\n",
        "    if use_gdrive:\n",
        "        folder = Path(gdrive_folder or Config.GDRIVE_FOLDER)\n",
        "        folder.mkdir(parents=True, exist_ok=True)\n",
        "        final_path = folder / Path(output_path).name\n",
        "    else:\n",
        "        final_path = Path(output_path)\n",
        "\n",
        "    # Sauvegarde WAV\n",
        "    with wave.open(str(final_path), \"wb\") as wav_file:\n",
        "        wav_file.setnchannels(1)\n",
        "        wav_file.setsampwidth(2)\n",
        "        wav_file.setframerate(Config.SAMPLE_RATE)\n",
        "        wav_file.writeframes(audio.tobytes())\n",
        "\n",
        "    duration = len(audio) / Config.SAMPLE_RATE\n",
        "    print(f\"âœ“ Audio genere: {final_path}\")\n",
        "    print(f\"  Duree: {duration:.2f}s | Voix: {voice}\")\n",
        "\n",
        "    return {\n",
        "        \"path\": str(final_path),\n",
        "        \"sample_rate\": Config.SAMPLE_RATE,\n",
        "        \"duration_seconds\": duration,\n",
        "        \"audio_data\": audio,\n",
        "        \"voice\": voice,\n",
        "        \"language\": language,\n",
        "        \"device\": _device_name,\n",
        "    }\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# UTILITIES\n",
        "# ==============================================================================\n",
        "\n",
        "\n",
        "def preview_audio(result: dict) -> None:\n",
        "    \"\"\"Previsualise l'audio dans le notebook.\"\"\"\n",
        "    from IPython.display import Audio, display\n",
        "\n",
        "    audio = result[\"audio_data\"]\n",
        "    if audio.dtype == np.int16:\n",
        "        audio = audio.astype(np.float32) / 32768.0\n",
        "    display(Audio(audio, rate=result[\"sample_rate\"]))\n",
        "\n",
        "\n",
        "def list_voices() -> list:\n",
        "    \"\"\"Liste les voix disponibles.\"\"\"\n",
        "    return list(Config.PRESET_VOICES.keys())\n",
        "\n",
        "\n",
        "def list_languages() -> list:\n",
        "    \"\"\"Liste les langues supportees.\"\"\"\n",
        "    return [\n",
        "        \"en\",\n",
        "        \"es\",\n",
        "        \"fr\",\n",
        "        \"de\",\n",
        "        \"it\",\n",
        "        \"pt\",\n",
        "        \"pl\",\n",
        "        \"tr\",\n",
        "        \"ru\",\n",
        "        \"nl\",\n",
        "        \"cs\",\n",
        "        \"ar\",\n",
        "        \"zh-cn\",\n",
        "        \"ja\",\n",
        "        \"hu\",\n",
        "        \"ko\",\n",
        "        \"hi\",\n",
        "    ]\n",
        "\n",
        "\n",
        "def clear_cache():\n",
        "    \"\"\"Vide le cache du modele.\"\"\"\n",
        "    global _tts_model\n",
        "    import torch\n",
        "\n",
        "    _tts_model = None\n",
        "    gc.collect()\n",
        "    if _device_name.startswith(\"cuda\"):\n",
        "        torch.cuda.empty_cache()\n",
        "    print(\"âœ“ Cache vide\")\n",
        "\n",
        "\n",
        "def estimate_duration(text: str) -> dict:\n",
        "    \"\"\"Estime la duree audio pour un texte.\"\"\"\n",
        "    duration = TextSplitter.estimate_audio_duration(text)\n",
        "    chunks = len(TextSplitter.split_for_long_audio(text))\n",
        "    return {\n",
        "        \"chars\": len(text),\n",
        "        \"estimated_seconds\": duration,\n",
        "        \"estimated_formatted\": ProgressTracker._format_time(duration),\n",
        "        \"chunks_estimate\": chunks,\n",
        "    }\n",
        "\n",
        "\n",
        "# Aliases\n",
        "tts = text_to_speech\n",
        "tts_long = text_to_speech_long\n",
        "\n",
        "# ==============================================================================\n",
        "# INITIALIZATION\n",
        "# ==============================================================================\n",
        "\n",
        "\n",
        "def init():\n",
        "    \"\"\"Initialise le module.\"\"\"\n",
        "    detect_device()\n",
        "    print(\"âœ… Module XTTS v2 Long Audio v4 charge\")\n",
        "    print(f\"   Device: {_device_name}\")\n",
        "    print(f\"   Voix disponibles: {list_voices()}\")\n",
        "    print(f\"   enable_text_splitting: active par defaut\")\n",
        "    print(f\"   Fix torchcodec: actif\")\n",
        "\n",
        "\n",
        "# Auto-init\n",
        "try:\n",
        "    detect_device()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TTS XTTS v2 - Long Audio Generator v4\")\n",
        "print(\"Compatible PyTorch 2.9+ (fix torchcodec)\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Voix disponibles: {list_voices()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mount_drive"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "â„¹ï¸ Google Colab non detecte ou Drive indisponible -> montage ignore.\n"
          ]
        }
      ],
      "source": [
        "# Monter Google Drive (optionnel)\n",
        "# Le notebook peut aussi s'executer hors Colab : dans ce cas on ignore simplement le montage.\n",
        "\n",
        "try:\n",
        "    from google.colab import drive  # type: ignore\n",
        "\n",
        "    drive.mount(\"/content/drive\")\n",
        "except Exception as e:\n",
        "    print(\"â„¹ï¸ Google Colab non detecte ou Drive indisponible -> montage ignore.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "init_module"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âš™ï¸ Device: cuda (Tesla T4)\n",
            "âœ… Module XTTS v2 Long Audio v4 charge\n",
            "   Device: cuda (Tesla T4)\n",
            "   Voix disponibles: ['female_fr', 'male_fr']\n",
            "   enable_text_splitting: active par defaut\n",
            "   Fix torchcodec: actif\n"
          ]
        }
      ],
      "source": [
        "# Initialisation du module\n",
        "init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# GEMINI PROSODY ENHANCER (NOUVEAU)\n",
        "# ==============================================================================\n",
        "\n",
        "class ProsodyEnhancer:\n",
        "    \"\"\"\n",
        "    Utilise l'API Gemini pour rÃ©Ã©crire le texte et amÃ©liorer la prosodie TTS.\n",
        "    \"\"\"\n",
        "    \n",
        "    SYSTEM_PROMPT = \"\"\"\n",
        "    Tu es un expert en ingÃ©nierie textuelle pour la synthÃ¨se vocale neuronale (TTS). \n",
        "    Ta tÃ¢che est de rÃ©Ã©crire le texte fourni pour qu'il soit lu de maniÃ¨re ultra-naturelle et humaine.\n",
        "    \n",
        "    CONSIGNES STRICTES :\n",
        "    1. PONCTUATION RESPIRATOIRE : Ajoute des virgules (,) pour les courtes respirations et des points de suspension (...) pour les pauses rÃ©flÃ©chies ou les fins de phrases douces. C'est le levier le plus important.\n",
        "    2. NOMBRES : Convertis TOUS les chiffres, dates, et pourcentages en toutes lettres (ex: \"2025\" -> \"deux mille vingt-cinq\").\n",
        "    3. SIGLES : SÃ©pare les lettres des sigles par des espaces si elles doivent Ãªtre Ã©pelÃ©es (ex: \"IA\" -> \"I A\", \"USA\" -> \"U S A\").\n",
        "    4. SEGMENTATION : Coupe les phrases trop longues en propositions plus digestes.\n",
        "    5. FIDÃ‰LITÃ‰ : Ne change pas le sens du texte. Ne supprime pas d'informations. Garde le style original.\n",
        "    6. FORMAT : Renvoie uniquement le texte brut modifiÃ©, sans balises markdown ni commentaires.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def enhance_text(text: str, api_key: str) -> str:\n",
        "        \"\"\"\n",
        "        Envoie le texte Ã  Gemini pour amÃ©lioration prosodique.\n",
        "        \"\"\"\n",
        "        if not api_key:\n",
        "            print(\"âš ï¸ Pas de clÃ© API Gemini fournie. Utilisation du texte brut.\")\n",
        "            return text\n",
        "            \n",
        "        import google.generativeai as genai\n",
        "        \n",
        "        print(\"\\nâœ¨ Appel Ã  Gemini pour amÃ©lioration de la prosodie...\")\n",
        "        try:\n",
        "            genai.configure(api_key=api_key)\n",
        "            model = genai.GenerativeModel(Config.GEMINI_MODEL)\n",
        "            \n",
        "            # Pour les textes trÃ¨s longs, on s'assure que Gemini peut gÃ©rer le contexte\n",
        "            # Gemini 1.5 a une Ã©norme fenÃªtre de contexte, donc on envoie tout d'un bloc\n",
        "            # pour la cohÃ©rence, sauf si c'est vraiment gigantesque.\n",
        "            \n",
        "            response = model.generate_content(\n",
        "                f\"{ProsodyEnhancer.SYSTEM_PROMPT}\\n\\nTEXTE Ã€ TRAITER :\\n{text}\"\n",
        "            )\n",
        "            \n",
        "            enhanced_text = response.text.strip()\n",
        "            print(\"âœ… Texte amÃ©liorÃ© avec succÃ¨s.\")\n",
        "            return enhanced_text\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Erreur Gemini : {e}\")\n",
        "            print(\"   -> Repli sur le texte original.\")\n",
        "            return text\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "â„¹ï¸  Colab UI non dÃ©tectÃ©e (VS Code ?). Recherche d'une alternative...\n",
            "âŒ Erreur : ClÃ© introuvable. Utilisez 'export GEMINI_API_KEY=...' dans votre terminal.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "def slugify(text):\n",
        "    \"\"\"Nettoie le texte pour en faire un nom de fichier valide (max 30 car.).\"\"\"\n",
        "    text = text.lower()\n",
        "    # On garde les caractÃ¨res alphanumÃ©riques\n",
        "    text = re.sub(r'[^a-z0-9\\s-]', '', text)\n",
        "    # On remplace les espaces par des underscores et on tronque\n",
        "    return re.sub(r'\\s+', '_', text).strip('_')[:30]\n",
        "\n",
        "\n",
        "def get_gemini_key():\n",
        "    # 1. Tentative via Colab Secrets (Uniquement si UI Web)\n",
        "    if 'google.colab' in sys.modules:\n",
        "        try:\n",
        "            from google.colab import userdata\n",
        "            # On utilise un timeout court pour ne pas bloquer VS Code\n",
        "            return userdata.get('GEMINI_API_KEY')\n",
        "        except Exception:\n",
        "            # Si on est sur VS Code via Colab, userdata.get Ã©choue ou timeout\n",
        "            print(\"â„¹ï¸  Colab UI non dÃ©tectÃ©e (VS Code ?). Recherche d'une alternative...\")\n",
        "\n",
        "    # 2. Tentative via Variable d'environnement (IdÃ©al pour VS Code / Local)\n",
        "    # Pensez Ã  faire : export GEMINI_API_KEY=\"votre_cle\" dans le terminal VS Code\n",
        "    key = os.getenv('GEMINI_API_KEY')\n",
        "    if key:\n",
        "        return key\n",
        "\n",
        "    # 3. Ultime recours : Saisie manuelle (bloquant mais efficace)\n",
        "    # print(\"âš ï¸ Aucune clÃ© trouvÃ©e dans les secrets ou l'environnement.\")\n",
        "    # return input(\"Veuillez coller votre clÃ© API Gemini ici : \").strip()\n",
        "    \n",
        "    return None\n",
        "\n",
        "# Utilisation\n",
        "GEMINI_KEY = get_gemini_key()\n",
        "\n",
        "if GEMINI_KEY:\n",
        "    print(\"âœ… ClÃ© GEMINI_API_KEY chargÃ©e.\")\n",
        "else:\n",
        "    print(\"âŒ Erreur : ClÃ© introuvable. Utilisez 'export GEMINI_API_KEY=...' dans votre terminal.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#GEMINI_API_KEY=\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "VKh0LumVWdsK"
      },
      "outputs": [],
      "source": [
        "# 1. Importation de la bibliothÃ¨que principale\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# 2. Importation de la fonction d'affichage (optionnel mais recommandÃ© pour la clartÃ©)\n",
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c0971d8a01540e99b31e6d749c7e31f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<h3>GÃ©nÃ©rateur TTS vers Drive</h3>'), Textarea(value='texte Ã  synthÃ©tiser', layout=â€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 3. INTERFACE UTILISATEUR (CORRIGÃ‰E)\n",
        "# ==========================================\n",
        "import os\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "DRIVE_FOLDER = \"/content/drive/MyDrive/Audio_Projet\"\n",
        "\n",
        "def slugify(text):\n",
        "    \"\"\"Nettoie le texte pour le nom de fichier (max 30 car.).\"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z0-9\\s-]', '', text)\n",
        "    return re.sub(r'\\s+', '_', text).strip('_')[:30]\n",
        "\n",
        "text_input = widgets.Textarea(\n",
        "    placeholder=\"Entrez votre texte ici...\",\n",
        "    value=\"texte Ã  synthÃ©tiser\",\n",
        "    layout=widgets.Layout(width=\"100%\", height=\"150px\"),\n",
        ")\n",
        "button = widgets.Button(\n",
        "    description=\"GÃ©nÃ©rer & Sauvegarder\", button_style=\"success\", icon=\"check\"\n",
        ")\n",
        "progress_bar = widgets.IntProgress(\n",
        "    value=0, min=0, max=100, layout=widgets.Layout(width=\"100%\", visibility=\"hidden\")\n",
        ")\n",
        "output = widgets.Output()\n",
        "\n",
        "def on_click(b):\n",
        "    output.clear_output()\n",
        "\n",
        "    with output:\n",
        "        texte_a_traiter = text_input.value.strip()\n",
        "        if not texte_a_traiter:\n",
        "            print(\"âŒ Le texte est vide.\")\n",
        "            return\n",
        "\n",
        "        button.disabled = True\n",
        "        progress_bar.layout.visibility = \"visible\"\n",
        "        progress_bar.value = 10\n",
        "\n",
        "        # 1. Humanisation et Titre\n",
        "        titre_pour_fichier = texte_a_traiter[:40]\n",
        "        if 'GEMINI_KEY' in globals() and GEMINI_KEY:\n",
        "            try:\n",
        "                texte_a_traiter = ProsodyEnhancer.enhance_text(texte_a_traiter, GEMINI_KEY)\n",
        "                titre_pour_fichier = texte_a_traiter[:40]\n",
        "                progress_bar.value = 30\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ Note : Erreur Gemini : {e}\")\n",
        "\n",
        "        try:\n",
        "            # 2. GÃ©nÃ©ration TTS\n",
        "            # La fonction renvoie un dictionnaire selon votre code source\n",
        "            result_dict = text_to_speech(texte_a_traiter, voice=\"female_fr\", enhance=True)\n",
        "            \n",
        "            # EXTRACTION DES DONNÃ‰ES AUDIO (La correction est ici)\n",
        "            audio_raw_data = result_dict[\"audio_data\"]\n",
        "            \n",
        "            progress_bar.value = 70\n",
        "\n",
        "            # PrÃ©visualisation\n",
        "            preview_audio(result_dict)\n",
        "\n",
        "            # 3. Sauvegarde sur Google Drive\n",
        "            if not os.path.exists(DRIVE_FOLDER):\n",
        "                os.makedirs(DRIVE_FOLDER)\n",
        "\n",
        "            nom_propre = slugify(titre_pour_fichier)\n",
        "            timestamp = datetime.now().strftime(\"%H%M%S\")\n",
        "            filename = f\"{nom_propre}_{timestamp}.wav\"\n",
        "            file_path = os.path.join(DRIVE_FOLDER, filename)\n",
        "\n",
        "            # Ã‰criture des bytes (audio_data est un numpy array converti en bytes)\n",
        "            with open(file_path, \"wb\") as f:\n",
        "                # On utilise .tobytes() car audio_data est un tableau numpy int16\n",
        "                f.write(audio_raw_data.tobytes())\n",
        "            \n",
        "            progress_bar.value = 100\n",
        "            progress_bar.bar_style = \"success\"\n",
        "            print(f\"âœ… Fichier sauvegardÃ© sur Drive : {filename}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            progress_bar.bar_style = \"danger\"\n",
        "            print(f\"âŒ Erreur lors du traitement : {e}\")\n",
        "        finally:\n",
        "            button.disabled = False\n",
        "\n",
        "button.on_click(on_click)\n",
        "\n",
        "display(widgets.VBox([widgets.HTML(\"<h3>GÃ©nÃ©rateur TTS vers Drive</h3>\"), text_input, button, progress_bar, output]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
